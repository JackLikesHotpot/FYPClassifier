{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import sklearn.metrics as metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Model\n",
      "F1 Score:  0.5996797437950361\n",
      "Number of instances correctly determined:  749\n",
      "\n",
      "Logistic Regression Model\n",
      "F1 Score:  0.6180944755804644\n",
      "Number of instances correctly determined:  772\n",
      "\n",
      "Support Vector Machine\n",
      "F1 Score:  0.6148919135308246\n",
      "Number of instances correctly determined:  768\n"
     ]
    }
   ],
   "source": [
    "labels = ['FAVOR', 'AGAINST', 'NEUTRAL']\n",
    "trainTweets = []\n",
    "trainStances = []\n",
    "testTweets = []\n",
    "testStances = []\n",
    "\n",
    "with open (\"trainingdata-all-annotations.txt\") as f:\n",
    "    for line in f:\n",
    "        if line.split('\\t')[2] != 'Tweet':\n",
    "            trainTweets.append(line.split('\\t')[2])#append train tweet\n",
    "            trainStances.append(line.split('\\t')[3]) #append train stance\n",
    "        \n",
    "            \n",
    "with open (\"testdata-taskA-all-annotations.txt\") as f:\n",
    "    for line in f:\n",
    "        if line.split('\\t')[2] != 'Tweet':\n",
    "            testTweets.append(line.split('\\t')[2])   #append test tweet\n",
    "            testStances.append(line.split('\\t')[3])  #append test stance\n",
    "            \n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train = vectorizer.fit_transform(trainTweets)\n",
    "X_test = vectorizer.transform(testTweets)\n",
    "\n",
    "runBayes(X_train, trainStances, X_test, testStances)\n",
    "print()\n",
    "runLogReg(X_train, trainStances, X_test, testStances)\n",
    "print()\n",
    "runSVM(X_train, trainStances, X_test, testStances)\n",
    "\n",
    "#CONFUSION MATRIX\n",
    "#matrix = (metrics.confusion_matrix(testStances, y_pred, labels))\n",
    "#fig = plt.figure()\n",
    "#ax = fig.add_subplot(111)\n",
    "#matrixAxis = ax.matshow(cm)\n",
    "#fig.colorbar(matrixAxis)\n",
    "#plt.title('Confusion matrix of classifier')\n",
    "#ax.set_xticklabels([''] + labels)\n",
    "#ax.set_yticklabels([''] + labels)\n",
    "#plt.xlabel('Predicted Label')\n",
    "#plt.ylabel('True Label')\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MULTINOMIAL NAIVE BAYES MODEL\n",
    "\n",
    "def runBayes(X_train, trainStances, X_test, testStances):\n",
    "    model = MultinomialNB()\n",
    "    model.fit(X_train, trainStances)\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    print(\"Naive Bayes Model\")\n",
    "    print(\"F1 Score: \", metrics.f1_score(testStances, y_pred, average='micro'))\n",
    "    print(\"Number of instances correctly determined: \", metrics.accuracy_score(testStances, y_pred, normalize=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOGISTIC REGRESSION MODEL\n",
    "\n",
    "def runLogReg(X_train, trainStances, X_test, testStances):\n",
    "    model = LogisticRegression(solver='lbfgs', multi_class='auto')\n",
    "    model.fit(X_train, trainStances)\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    print(\"Logistic Regression Model\")\n",
    "    print(\"F1 Score: \", metrics.f1_score(testStances, y_pred, average='micro'))\n",
    "    print(\"Number of instances correctly determined: \", metrics.accuracy_score(testStances, y_pred, normalize=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SUPPORT VECTOR MACHINE MODEL\n",
    "\n",
    "def runSVM(X_train, trainStances, X_test, testStances):\n",
    "    model = SVC(kernel='linear', C=1, gamma=1)\n",
    "    model.fit(X_train, trainStances)\n",
    "    y_pred = model.predict(X_test)\n",
    "     \n",
    "    print(\"Support Vector Machine\")\n",
    "    print(\"F1 Score: \", metrics.f1_score(testStances, y_pred, average='micro'))\n",
    "    print(\"Number of instances correctly determined: \", metrics.accuracy_score(testStances, y_pred, normalize=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeStopWords(tweet):\n",
    "    result = []\n",
    "    joined = \"\"\n",
    "    stopWords = set(stopwords.words('english'))\n",
    "    for word in tweet.split(' '):\n",
    "        if word not in stopWords:\n",
    "            result.append(word)\n",
    "    joined = ' '.join(result)    \n",
    "    return joined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatizeTweet(tweet):\n",
    "    result = []\n",
    "    joined = \"\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    for word in tweet.split(' '):\n",
    "        result.append(lemmatizer.lemmatize(word.lower()))\n",
    "    joined = ' '.join(result)\n",
    "    return joined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Implement decision trees?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 't_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-30f0ca97f4f1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msent_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mt_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Stance'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msent_count\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msent_count\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Tweet'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Stance'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Number of Tweets'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 't_data' is not defined"
     ]
    }
   ],
   "source": [
    "sent_count = t_data.groupby('Stance').count()\n",
    "plt.bar(sent_count.index.values, sent_count['Tweet'])\n",
    "plt.xlabel('Stance')\n",
    "plt.ylabel('Number of Tweets')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#twokenize\n",
    "#feature extraction to get vector representation of text\n",
    "\n",
    "#tfidf\n",
    "#https://scikit-learn.org/stable/modules/feature_extraction.html\n",
    "#insert vectors into classifiers"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
